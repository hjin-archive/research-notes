---
title: Linear Algebra
---

# Linear Algebra

A compact, applied set of notes: the pieces that show up constantly in ML, optimization, and numerical stability.

## Pages
- [Projections](/notes/linear-algebra/projections) — geometry of least squares, orthogonality, and “why it works”
- [SVD](/notes/linear-algebra/svd) — low-rank structure, PCA connection, and what it explains about models
- *(Add more as you go: eigendecomposition, conditioning, spectral norms, etc.)*

## What to remember
- Most “mysteries” reduce to projections onto subspaces.
- Conditioning often explains instability more than “model complexity”.
- SVD is the reusable lens: compression, denoising, PCA, inverse problems.
